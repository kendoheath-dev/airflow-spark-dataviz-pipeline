FROM apache/airflow:2.10.4
# FROM apache/airflow:latest-python3.10
WORKDIR /opt/airflow
ENV AIRFLOW_HOME=/opt/airflow

RUN pip install --upgrade pip && \
    pip install apache-airflow-providers-apache-spark

USER root
# Install Java and other prerequisites to allow spark-submit to run
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        curl \
        iputils-ping \
        default-jdk \
        wget \
        ca-certificates && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*
# Download and install Spark. using archived binary
RUN curl -fsSL -o /tmp/spark.tgz https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz && \
    tar -xzf /tmp/spark.tgz -C /opt && \
    ln -s /opt/spark-3.5.1-bin-hadoop3 /opt/spark && \
    ln -s /opt/spark/bin/spark-submit /usr/local/bin/spark-submit && \
    rm /tmp/spark.tgz
RUN wget https://jdbc.postgresql.org/download/postgresql-42.7.3.jar -P /opt/bitnami/spark/jars

# Set Spark environment variables
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH
USER airflow
















# RUN pip install --no-cache-dir apache-airflow-providers-postgres==5.7.0
# RUN pip install apache-airflow-providers-postgres --constraint \
# "https://raw.githubusercontent.com/apache/airflow/constraints-2.10.0/constraints-3.10.txt"


# ENV PYTHONPATH=/opt/airflow:$PYTHONPATH
# ENV PYTHONPATH="/opt/airflow/:${PYTHONPATH}"

# ENV VIRTUAL_ENV=/opt/airflow/venv
# ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# RUN python -m venv $VIRTUAL_ENV
# RUN $VIRTUAL_ENV/bin/pip install --upgrade pip
# RUN $VIRTUAL_ENV/bin/pip install apache-airflow-providers-postgres --constraint \
# "https://raw.githubusercontent.com/apache/airflow/constraints-2.10.0/constraints-3.10.txt"